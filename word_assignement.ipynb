{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1558039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configs\n",
    "GPT2_small = {\n",
    "    \"num_layers\": 12,\n",
    "    \"d_model\": 768,\n",
    "    \"num_heads\": 12,\n",
    "    \"d_ff\": 4 * 768,   # 3072\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024\n",
    "}\n",
    "\n",
    "GPT2_medium = {\n",
    "    \"num_layers\": 24,\n",
    "    \"d_model\": 1024,\n",
    "    \"num_heads\": 16,\n",
    "    \"d_ff\": 4 * 1024,  # 4096\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024\n",
    "}\n",
    "\n",
    "GPT2_large = {\n",
    "    \"num_layers\": 36,\n",
    "    \"d_model\": 1280,\n",
    "    \"num_heads\": 20,\n",
    "    \"d_ff\": 4 * 1280,  # 5120\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024\n",
    "}\n",
    "\n",
    "GPT2_XL = {\n",
    "    \"num_layers\": 48,\n",
    "    \"d_model\": 1600,\n",
    "    \"num_heads\": 25,\n",
    "    \"d_ff\": 4 * 1600,  # 6400\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2_small\": GPT2_small,\n",
    "    \"gpt2_medium\": GPT2_medium,\n",
    "    \"gpt2_large\": GPT2_large,\n",
    "    \"gpt2_xl\": GPT2_XL,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702ad0c",
   "metadata": {},
   "source": [
    "## 3. Transformer LM resource accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba22a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2-XL config\n",
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cee635",
   "metadata": {},
   "source": [
    "### a) memeory accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff13f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,127,057,600\n"
     ]
    }
   ],
   "source": [
    "# Calculate manually\n",
    "n_rms_norm = d_model\n",
    "\n",
    "# Token embedding\n",
    "n_token_embedding = vocab_size * d_model\n",
    "\n",
    "# MHA\n",
    "n_wq = d_model * num_heads * (d_model // num_heads)\n",
    "n_wk = d_model * num_heads * (d_model // num_heads)\n",
    "n_wv = d_model * num_heads * (d_model // num_heads)\n",
    "n_wo = num_heads * (d_model // num_heads) * d_model\n",
    "n_mha = n_wq + n_wk + n_wv + n_wq\n",
    "# FFN\n",
    "n_w1 = d_model * d_ff\n",
    "n_w3 = d_model * d_ff\n",
    "n_w2 = d_ff * d_model\n",
    "n_ffn = n_w1 + n_w2 + n_w3\n",
    "\n",
    "# trf_block\n",
    "n_trf_block = n_rms_norm * 2 + n_mha + n_ffn\n",
    "\n",
    "# Output\n",
    "n_out_proj = d_model * vocab_size\n",
    "\n",
    "total = n_token_embedding + n_trf_block * num_layers + n_rms_norm + n_out_proj\n",
    "print(f\"{total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8035dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate with functions\n",
    "from cs336_basics.assignment_utils import get_model_parameter\n",
    "from cs336_basics.model import *\n",
    "\n",
    "lm = TransformerLM(\n",
    "    vocab_size,\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    context_length,\n",
    "    10000,\n",
    "    num_layers\n",
    ")\n",
    "\n",
    "num_paraters = get_model_parameter(lm)\n",
    "print(f\"{num_paraters:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60280cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2,127,057,600.\n",
      "Size of parameters: 7.92 GB.\n",
      "Number of parameters in training (with gradients): 4,254,115,200.\n",
      "Size of parameters in training (with gradients): 15.85 GB.\n",
      "Number of buffers: 6,291,456.\n",
      "Size of buffers: 0.02 GB.\n"
     ]
    }
   ],
   "source": [
    "# Calculate memory\n",
    "from cs336_basics.assignment_utils import get_model_size\n",
    "\n",
    "get_model_size(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddae7e0",
   "metadata": {},
   "source": [
    "### b) Compute FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bee853ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA\n",
    "\n",
    "FLOPs = {}\n",
    "# Linear Projection\n",
    "FLOPs[\"mha_linear\"] = 2*context_length*d_model*d_model\n",
    "# Compute attention\n",
    "FLOPs[\"mha_attn_weight\"] = 2*context_length*d_model*context_length\n",
    "FLOPs[\"mha_context_vector\"] = 2*context_length*context_length*d_model\n",
    "# Output projection\n",
    "FLOPs[\"mha_output_linear\"] = 2*context_length*d_model*d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd8f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFN\n",
    "\n",
    "FLOPs[\"ffn_linear1\"] = 2*context_length*d_model*d_ff\n",
    "FLOPs[\"ffn_linear3\"] = 2*context_length*d_model*d_ff\n",
    "FLOPs[\"ffn_linear2\"] = 2*context_length*d_ff*d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a17ab5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer_block\n",
    "qkv = 3\n",
    "ffn_linears = 3\n",
    "FLOPs[\"transformer_block\"] = FLOPs[\"mha_linear\"] * qkv \\\n",
    "                            + FLOPs[\"mha_attn_weight\"]  \\\n",
    "                            + FLOPs[\"mha_context_vector\"] \\\n",
    "                            + FLOPs[\"mha_output_linear\"] \\\n",
    "                            + FLOPs[\"ffn_linear1\"] * ffn_linears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93ba45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final linear projection\n",
    "FLOPs[\"final_linear\"] = 2*context_length*d_model*vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a074b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total FLOPs: 4,513,336,524,800\n",
      "approximate total FLOPs: 4.51e+12\n"
     ]
    }
   ],
   "source": [
    "# Total FLOPs in the model\n",
    "total_FLOPs = num_layers * (FLOPs[\"transformer_block\"]) + FLOPs[\"final_linear\"]\n",
    "print(f\"total FLOPs: {total_FLOPs:,}\")\n",
    "print(f\"approximate total FLOPs: {total_FLOPs:.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef2d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from thop import profile\n",
    "\n",
    "# def get_model_FLOPs(model: nn.Module, token_ids: Int[Tensor, \"... seq_len\"]):\n",
    "#     macs, params = profile(model, (token_ids,))\n",
    "#     flops = 2 * macs\n",
    "\n",
    "#     print(f\"Total FLOPs of the model and input: {flops:,}\")\n",
    "#     print(f\"Approximated total FLOPs of the model and input: {flops:.2e}\")\n",
    "\n",
    "# token_ids = torch.randint(low=0, high=vocab_size, size=(1, context_length,))\n",
    "# get_model_FLOPs(lm, token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3b6ea8",
   "metadata": {},
   "source": [
    "### c) most FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1f4410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mha_attn_weight', 3355443200),\n",
       " ('mha_context_vector', 3355443200),\n",
       " ('mha_linear', 5242880000),\n",
       " ('mha_output_linear', 5242880000),\n",
       " ('ffn_linear1', 20971520000),\n",
       " ('ffn_linear3', 20971520000),\n",
       " ('ffn_linear2', 20971520000),\n",
       " ('transformer_block', 90596966400),\n",
       " ('final_linear', 164682137600)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLOPs_sorted = sorted(FLOPs.items(), key=lambda x: x[1])\n",
    "FLOPs_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c82842e",
   "metadata": {},
   "source": [
    "### d) FLOPs proportion varying with model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e1ffab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flops_proportion(model_config: dict, context_length: int):\n",
    "    '''\n",
    "    Breakdown component: ((MHA, FFN) -> transformer_block) * num_layers -> trf_blocks, final linear\n",
    "    '''\n",
    "    # MHA\n",
    "    flops_mha_linear = 2 * context_length * model_config[\"d_model\"] * model_config[\"d_model\"]\n",
    "    flops_mha_attn_weight = 2 * context_length * context_length * model_config[\"d_model\"]\n",
    "    flops_mha_context_vector = 2 * context_length * context_length * model_config[\"d_model\"]\n",
    "    flops_mha_output_linear = 2 * context_length * model_config[\"d_model\"] * model_config[\"d_model\"]\n",
    "\n",
    "    qkv = 3\n",
    "    flops_mha = qkv * flops_mha_linear + flops_mha_attn_weight + flops_mha_context_vector + flops_mha_output_linear\n",
    "\n",
    "    # FFN\n",
    "    flops_ffn_linear1 = 2 * context_length * model_config[\"d_model\"] * model_config[\"d_ff\"]\n",
    "    flops_ffn_linear2 = 2 * context_length * model_config[\"d_model\"] * model_config[\"d_ff\"]\n",
    "    flops_ffn_linear3 = 2 * context_length * model_config[\"d_model\"] * model_config[\"d_ff\"]\n",
    "\n",
    "    ffn_linears = 3\n",
    "    flops_ffn = ffn_linears * flops_ffn_linear1\n",
    "\n",
    "    # Transformer block(s)\n",
    "    flops_transformer_block = flops_mha + flops_ffn\n",
    "    flops_trf_blocks = model_config[\"num_layers\"] * flops_transformer_block\n",
    "\n",
    "    # Final linear projection\n",
    "    flops_final_linear = 2 * context_length * model_config[\"d_model\"] * model_config[\"vocab_size\"]\n",
    "\n",
    "    total_flops = flops_trf_blocks + flops_final_linear\n",
    "\n",
    "    print(f\"FLOPs proportion of MHA is {(flops_mha / total_flops) * 100:.2f}%\")\n",
    "    print(f\"FLOPs proportion of FFN is {(flops_ffn / total_flops) * 100:.2f}%\")\n",
    "    print(f\"FLOPs proportion of transformer_block is {(flops_transformer_block / total_flops) * 100:.2f}%\")\n",
    "    print(f\"FLOPs proportion of all trf_blocks is {(flops_trf_blocks / total_flops) * 100:.2f}%\")\n",
    "    print(f\"FLOPs proportion of all final linear projection is {(flops_final_linear / total_flops) * 100:.2f}%\")\n",
    "\n",
    "def compute_models_flops(model_configs: dict):\n",
    "    context_length = 1024\n",
    "\n",
    "    for model_size, cfg in model_configs.items():\n",
    "        print(f\"---------------{model_size}--------------\")\n",
    "        compute_flops_proportion(cfg, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b871b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------gpt2_small--------------\n",
      "FLOPs proportion of MHA is 2.30%\n",
      "FLOPs proportion of FFN is 4.15%\n",
      "FLOPs proportion of transformer_block is 6.45%\n",
      "FLOPs proportion of all trf_blocks is 77.39%\n",
      "FLOPs proportion of all final linear projection is 22.61%\n",
      "---------------gpt2_medium--------------\n",
      "FLOPs proportion of MHA is 1.25%\n",
      "FLOPs proportion of FFN is 2.49%\n",
      "FLOPs proportion of transformer_block is 3.74%\n",
      "FLOPs proportion of all trf_blocks is 89.80%\n",
      "FLOPs proportion of all final linear projection is 10.20%\n",
      "---------------gpt2_large--------------\n",
      "FLOPs proportion of MHA is 0.83%\n",
      "FLOPs proportion of FFN is 1.78%\n",
      "FLOPs proportion of transformer_block is 2.62%\n",
      "FLOPs proportion of all trf_blocks is 94.16%\n",
      "FLOPs proportion of all final linear projection is 5.84%\n",
      "---------------gpt2_xl--------------\n",
      "FLOPs proportion of MHA is 0.61%\n",
      "FLOPs proportion of FFN is 1.39%\n",
      "FLOPs proportion of transformer_block is 2.01%\n",
      "FLOPs proportion of all trf_blocks is 96.35%\n",
      "FLOPs proportion of all final linear projection is 3.65%\n"
     ]
    }
   ],
   "source": [
    "compute_models_flops(model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870babae",
   "metadata": {},
   "source": [
    "### 5) Increase context_length for FLOPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f78ac5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs proportion of MHA is 0.61%\n",
      "FLOPs proportion of FFN is 1.39%\n",
      "FLOPs proportion of transformer_block is 2.01%\n",
      "FLOPs proportion of all trf_blocks is 96.35%\n",
      "FLOPs proportion of all final linear projection is 3.65%\n",
      "--------------------------\n",
      "FLOPs proportion of MHA is 1.37%\n",
      "FLOPs proportion of FFN is 0.67%\n",
      "FLOPs proportion of transformer_block is 2.05%\n",
      "FLOPs proportion of all trf_blocks is 98.24%\n",
      "FLOPs proportion of all final linear projection is 1.76%\n"
     ]
    }
   ],
   "source": [
    "compute_flops_proportion(GPT2_XL, context_length=1024)\n",
    "print(\"--------------------------\")\n",
    "compute_flops_proportion(GPT2_XL, context_length=16384)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d15f29",
   "metadata": {},
   "source": [
    "## 4. Training resource accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fe7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c541e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cs336_basics.utils' from 'e:\\\\LLM\\\\CS336\\\\assignment1-basics\\\\cs336_basics\\\\utils.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import cs336_basics.assignment_utils\n",
    "importlib.reload(cs336_basics.assignment_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0addbd5c",
   "metadata": {},
   "source": [
    "### a) Peak memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58eac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of parameters is 7.92 GB\n",
      "Size of activations is 57.83 GB\n",
      "Size of grads is 7.92 GB\n",
      "Size of optimizer state is 15.85 GB\n",
      "Estimated peak memory of the model is 89.53 GB\n",
      "14.458137512207031\n",
      "31.69562816619873\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.assignment_utils import calculate_model_peak_memory\n",
    "\n",
    "calculate_model_peak_memory(GPT2_XL, \"adamw\", 4, torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac102951",
   "metadata": {},
   "source": [
    "### b) Estimate max batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of parameters is 7.92 GB\n",
      "Size of activations is 14.46 GB\n",
      "Size of grads is 7.92 GB\n",
      "Size of optimizer state is 15.85 GB\n",
      "Estimated peak memory of the model is 46.15 GB\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.assignment_utils import calculate_max_batch\n",
    "\n",
    "memory_size_gb = 80\n",
    "max_batch_size = calculate_max_batch(\n",
    "    GPT2_XL,\n",
    "    memory_size_gb\n",
    ")\n",
    "print(max_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fee089",
   "metadata": {},
   "source": [
    "### c) FLOPs running optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdfe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs of running one step of the AdamW optimizer is 2.98e+10\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.assignment_utils import calculate_optimizer_flops_step\n",
    "\n",
    "opt_flops_per_step = calculate_optimizer_flops_step(GPT2_XL)\n",
    "print(f\"FLOPs of running one step of the AdamW optimizer is {opt_flops_per_step:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847247de",
   "metadata": {},
   "source": [
    "### d) Training time estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff3227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated training days is 6583.57055222792 days.\n"
     ]
    }
   ],
   "source": [
    "from cs336_basics.assignment_utils import estimate_train_days\n",
    "\n",
    "estimate_train_days(\n",
    "    GPT2_XL,\n",
    "    hardware_flops_tera=19.5,\n",
    "    mfu=0.5,\n",
    "    train_steps=400000,\n",
    "    batch_size=1024\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment1-basics (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
